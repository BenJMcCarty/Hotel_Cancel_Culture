{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cancel Culture - Classification Modeling Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Author:** Ben McCarty\n",
    "\n",
    "---\n",
    "\n",
    "**Email:** [bmccarty505@gmail.com](mailto:bmccarty505@gmail.com)\n",
    "\n",
    "**LinkedIn:** [http://www.linkedin.com/in/bmccarty505](http://www.linkedin.com/in/bmccarty505)\n",
    "\n",
    "**GitHub:** [https://github.com/BenJMcCarty](https://github.com/BenJMcCarty)\n",
    "\n",
    "**Website:** [https://benmccarty.godaddysites.com](https://benmccarty.godaddysites.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Simplified Data Set**\n",
    "\n",
    "> The initial classification modeling notebook utilized all features of the full dataset. I decided that the details provided by that dataset were too specific to the two hotels from which the data was sourced.\n",
    ">\n",
    "> **To increase the generalizability of the results, I will focus on a selection of features that are more commonly available based on my professional experience.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Cancel-Culture---Classification-Modeling-Notebook\" data-toc-modified-id=\"Cancel-Culture---Classification-Modeling-Notebook-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><strong>Cancel Culture - Classification Modeling Notebook</strong></a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span><strong>Imports</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Speeding-Up-Scikit-Learn\" data-toc-modified-id=\"Speeding-Up-Scikit-Learn-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span><em>Speeding-Up Scikit-Learn</em></a></span></li></ul></li><li><span><a href=\"#Reading-the-DataFrames\" data-toc-modified-id=\"Reading-the-DataFrames-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><strong>Reading the DataFrames</strong></a></span></li><li><span><a href=\"#Creating-the-Subset\" data-toc-modified-id=\"Creating-the-Subset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Creating the Subset</a></span></li><li><span><a href=\"#Train/Test-Split\" data-toc-modified-id=\"Train/Test-Split-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span><strong>Train/Test Split</strong></a></span></li><li><span><a href=\"#Prepping-the-Pipeline\" data-toc-modified-id=\"Prepping-the-Pipeline-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><strong>Prepping the Pipeline</strong></a></span></li><li><span><a href=\"#Resampling-via-SMOTE\" data-toc-modified-id=\"Resampling-via-SMOTE-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span><strong>Resampling via SMOTE</strong></a></span></li><li><span><a href=\"#Baseline-Model\" data-toc-modified-id=\"Baseline-Model-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span><strong>Baseline Model</strong></a></span></li><li><span><a href=\"#Target-Metric:-Average-Precision\" data-toc-modified-id=\"Target-Metric:-Average-Precision-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span><strong>Target Metric: Average Precision</strong></a></span></li><li><span><a href=\"#Logistic-Regression-Model\" data-toc-modified-id=\"Logistic-Regression-Model-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span><strong>Logistic Regression Model</strong></a></span></li><li><span><a href=\"#Random-Forest-Model\" data-toc-modified-id=\"Random-Forest-Model-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span><strong>Random Forest Model</strong></a></span></li><li><span><a href=\"#ExtraTreesClassifier\" data-toc-modified-id=\"ExtraTreesClassifier-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span><strong>ExtraTreesClassifier</strong></a></span></li><li><span><a href=\"#XGBoost-Classifier\" data-toc-modified-id=\"XGBoost-Classifier-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span><strong>XGBoost Classifier</strong></a></span></li><li><span><a href=\"#GridSearches-and-Results\" data-toc-modified-id=\"GridSearches-and-Results-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span><strong>GridSearches and Results</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#GS:-Logistic-Regression\" data-toc-modified-id=\"GS:-Logistic-Regression-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>GS: Logistic Regression</a></span></li><li><span><a href=\"#GS:-Random-Forest\" data-toc-modified-id=\"GS:-Random-Forest-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;</span>GS: Random Forest</a></span></li><li><span><a href=\"#GS:-Extra-Trees-Classifier\" data-toc-modified-id=\"GS:-Extra-Trees-Classifier-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;</span>GS: Extra Trees Classifier</a></span></li></ul></li><li><span><a href=\"#Collecting-Coeffcients,-Inspecting-Importances\" data-toc-modified-id=\"Collecting-Coeffcients,-Inspecting-Importances-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span><strong>Collecting Coeffcients, Inspecting Importances</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspecting-Importances\" data-toc-modified-id=\"Inspecting-Importances-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>Inspecting Importances</a></span></li><li><span><a href=\"#Coefficients\" data-toc-modified-id=\"Coefficients-15.2\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;</span><strong>Coefficients</strong></a></span></li></ul></li><li><span><a href=\"#Interpreting-Results-with-SHAP\" data-toc-modified-id=\"Interpreting-Results-with-SHAP-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span><strong>Interpreting Results with SHAP</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-TreeExplainer\" data-toc-modified-id=\"Using-TreeExplainer-16.1\"><span class=\"toc-item-num\">16.1&nbsp;&nbsp;</span>Using TreeExplainer</a></span></li><li><span><a href=\"#Dot-Plot\" data-toc-modified-id=\"Dot-Plot-16.2\"><span class=\"toc-item-num\">16.2&nbsp;&nbsp;</span>Dot Plot</a></span></li></ul></li><li><span><a href=\"#Reviewing-Results\" data-toc-modified-id=\"Reviewing-Results-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>Reviewing Results</a></span></li><li><span><a href=\"#Recommendations\" data-toc-modified-id=\"Recommendations-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>Recommendations</a></span></li><li><span><a href=\"#Future-Work\" data-toc-modified-id=\"Future-Work-19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span>Future Work</a></span></li><li><span><a href=\"#Addendum:-Probabilities\" data-toc-modified-id=\"Addendum:-Probabilities-20\"><span class=\"toc-item-num\">20&nbsp;&nbsp;</span>Addendum: Probabilities</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:44:40.149861Z",
     "start_time": "2021-10-13T18:44:40.054482Z"
    }
   },
   "outputs": [],
   "source": [
    "## Jupyter Notebook setting to reload functions when called\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:44:57.265563Z",
     "start_time": "2021-10-13T18:44:40.153862Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Visualizing random forest classifier model results\n",
    "import shap\n",
    "\n",
    "## Personal functions\n",
    "from bmc_functions import classification as clf\n",
    "\n",
    "## SKLearn and Modeling Tools\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import set_config\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:44:57.585574Z",
     "start_time": "2021-10-13T18:44:57.268603Z"
    }
   },
   "outputs": [],
   "source": [
    "## Settings\n",
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.25)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('max_rows', 50)\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Speeding-Up Scikit-Learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Due to the size of my dataset, the modeling process took a fair amount of time, especially when testing different model types. To improve my models' runtime, I use a package called \"**Intel(R) Extension for Scikit-learn*.**\"\n",
    "\n",
    "This package operates in the background to increase the computational efficiency of certain Scikit-Learn models, including Logistic Regression and Random Forest Classifier models. The package does not affect the model results, though.\n",
    "\n",
    "This package requires the models to be imported after the package itself in order to perform the patching that results in better run-times.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:44:58.885589Z",
     "start_time": "2021-10-13T18:44:57.588575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "## Speeding up SKLearn via Intel(R) Extension for Scikit-learn*\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:44:59.182592Z",
     "start_time": "2021-10-13T18:44:58.890589Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inmporting models post-sklearn-intelex\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reading the DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In my prior EDA notebook, I reviewed, cleaned, and performed some pre-processing steps to prepare my data separately before modeling. I saved the data as a .pickle file to preserve the datatypes; now I will re-read the data for modeling purposes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:45:40.009895Z",
     "start_time": "2021-10-13T18:45:39.549862Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel</th>\n",
       "      <th>is_canceled</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_date_month</th>\n",
       "      <th>arrival_date_week_number</th>\n",
       "      <th>arrival_date_day_of_month</th>\n",
       "      <th>stays_in_weekend_nights</th>\n",
       "      <th>stays_in_week_nights</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>babies</th>\n",
       "      <th>meal</th>\n",
       "      <th>country</th>\n",
       "      <th>market_segment</th>\n",
       "      <th>is_repeated_guest</th>\n",
       "      <th>previous_cancellations</th>\n",
       "      <th>previous_bookings_not_canceled</th>\n",
       "      <th>reserved_room_type</th>\n",
       "      <th>assigned_room_type</th>\n",
       "      <th>booking_changes</th>\n",
       "      <th>deposit_type</th>\n",
       "      <th>days_in_waiting_list</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>adr</th>\n",
       "      <th>required_car_parking_spaces</th>\n",
       "      <th>total_of_special_requests</th>\n",
       "      <th>agent_group</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>arrival_day</th>\n",
       "      <th>stay_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resort Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>342</td>\n",
       "      <td>July</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>PRT</td>\n",
       "      <td>Direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Resort Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>737</td>\n",
       "      <td>July</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>PRT</td>\n",
       "      <td>Direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Resort Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>July</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>75.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Resort Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>July</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>75.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Resort Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>July</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Online TA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>98.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119385</th>\n",
       "      <td>City Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>August</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>Other</td>\n",
       "      <td>Offline TA/TO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>96.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119386</th>\n",
       "      <td>City Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>August</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>FRA</td>\n",
       "      <td>Online TA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>225.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119387</th>\n",
       "      <td>City Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>August</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>DEU</td>\n",
       "      <td>Online TA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>157.71</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119388</th>\n",
       "      <td>City Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>August</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>BB</td>\n",
       "      <td>GBR</td>\n",
       "      <td>Online TA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>104.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119389</th>\n",
       "      <td>City Hotel</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>August</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>HB</td>\n",
       "      <td>DEU</td>\n",
       "      <td>Online TA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>No Deposit</td>\n",
       "      <td>0</td>\n",
       "      <td>Transient</td>\n",
       "      <td>151.20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119388 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               hotel  is_canceled  lead_time arrival_date_month  \\\n",
       "0       Resort Hotel            0        342               July   \n",
       "1       Resort Hotel            0        737               July   \n",
       "2       Resort Hotel            0          7               July   \n",
       "3       Resort Hotel            0         13               July   \n",
       "4       Resort Hotel            0         14               July   \n",
       "...              ...          ...        ...                ...   \n",
       "119385    City Hotel            0         23             August   \n",
       "119386    City Hotel            0        102             August   \n",
       "119387    City Hotel            0         34             August   \n",
       "119388    City Hotel            0        109             August   \n",
       "119389    City Hotel            0        205             August   \n",
       "\n",
       "        arrival_date_week_number  arrival_date_day_of_month  \\\n",
       "0                             27                          1   \n",
       "1                             27                          1   \n",
       "2                             27                          1   \n",
       "3                             27                          1   \n",
       "4                             27                          1   \n",
       "...                          ...                        ...   \n",
       "119385                        35                         30   \n",
       "119386                        35                         31   \n",
       "119387                        35                         31   \n",
       "119388                        35                         31   \n",
       "119389                        35                         29   \n",
       "\n",
       "        stays_in_weekend_nights  stays_in_week_nights  adults  children  \\\n",
       "0                             0                     0       2      0.00   \n",
       "1                             0                     0       2      0.00   \n",
       "2                             0                     1       1      0.00   \n",
       "3                             0                     1       1      0.00   \n",
       "4                             0                     2       2      0.00   \n",
       "...                         ...                   ...     ...       ...   \n",
       "119385                        2                     5       2      0.00   \n",
       "119386                        2                     5       3      0.00   \n",
       "119387                        2                     5       2      0.00   \n",
       "119388                        2                     5       2      0.00   \n",
       "119389                        2                     7       2      0.00   \n",
       "\n",
       "        babies meal country market_segment  is_repeated_guest  \\\n",
       "0            0   BB     PRT         Direct                  0   \n",
       "1            0   BB     PRT         Direct                  0   \n",
       "2            0   BB     GBR         Direct                  0   \n",
       "3            0   BB     GBR      Corporate                  0   \n",
       "4            0   BB     GBR      Online TA                  0   \n",
       "...        ...  ...     ...            ...                ...   \n",
       "119385       0   BB   Other  Offline TA/TO                  0   \n",
       "119386       0   BB     FRA      Online TA                  0   \n",
       "119387       0   BB     DEU      Online TA                  0   \n",
       "119388       0   BB     GBR      Online TA                  0   \n",
       "119389       0   HB     DEU      Online TA                  0   \n",
       "\n",
       "        previous_cancellations  previous_bookings_not_canceled  \\\n",
       "0                            0                               0   \n",
       "1                            0                               0   \n",
       "2                            0                               0   \n",
       "3                            0                               0   \n",
       "4                            0                               0   \n",
       "...                        ...                             ...   \n",
       "119385                       0                               0   \n",
       "119386                       0                               0   \n",
       "119387                       0                               0   \n",
       "119388                       0                               0   \n",
       "119389                       0                               0   \n",
       "\n",
       "       reserved_room_type assigned_room_type  booking_changes deposit_type  \\\n",
       "0                       C                  C                3   No Deposit   \n",
       "1                       C                  C                4   No Deposit   \n",
       "2                       A                  C                0   No Deposit   \n",
       "3                       A                  A                0   No Deposit   \n",
       "4                       A                  A                0   No Deposit   \n",
       "...                   ...                ...              ...          ...   \n",
       "119385                  A                  A                0   No Deposit   \n",
       "119386                  E                  E                0   No Deposit   \n",
       "119387                  D                  D                0   No Deposit   \n",
       "119388                  A                  A                0   No Deposit   \n",
       "119389                  A                  A                0   No Deposit   \n",
       "\n",
       "        days_in_waiting_list customer_type    adr  \\\n",
       "0                          0     Transient   0.00   \n",
       "1                          0     Transient   0.00   \n",
       "2                          0     Transient  75.00   \n",
       "3                          0     Transient  75.00   \n",
       "4                          0     Transient  98.00   \n",
       "...                      ...           ...    ...   \n",
       "119385                     0     Transient  96.14   \n",
       "119386                     0     Transient 225.43   \n",
       "119387                     0     Transient 157.71   \n",
       "119388                     0     Transient 104.40   \n",
       "119389                     0     Transient 151.20   \n",
       "\n",
       "        required_car_parking_spaces  total_of_special_requests agent_group  \\\n",
       "0                                 0                          0           0   \n",
       "1                                 0                          0           0   \n",
       "2                                 0                          0           0   \n",
       "3                                 0                          0         999   \n",
       "4                                 0                          1         240   \n",
       "...                             ...                        ...         ...   \n",
       "119385                            0                          0         999   \n",
       "119386                            0                          2           9   \n",
       "119387                            0                          4           9   \n",
       "119388                            0                          0         999   \n",
       "119389                            0                          2           9   \n",
       "\n",
       "       arrival_date arrival_day  stay_length  \n",
       "0        2015-07-01   Wednesday            0  \n",
       "1        2015-07-01   Wednesday            0  \n",
       "2        2015-07-01   Wednesday            1  \n",
       "3        2015-07-01   Wednesday            1  \n",
       "4        2015-07-01   Wednesday            2  \n",
       "...             ...         ...          ...  \n",
       "119385   2017-08-30   Wednesday            7  \n",
       "119386   2017-08-31    Thursday            7  \n",
       "119387   2017-08-31    Thursday            7  \n",
       "119388   2017-08-31    Thursday            7  \n",
       "119389   2017-08-29     Tuesday            9  \n",
       "\n",
       "[119388 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('./data/data_prepped.pickle',\n",
    "                           compression = 'gzip')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Selecting Features for Subset**\n",
    "\n",
    "> As mentioned in my intro, I want to increase the generalizability of the data and associated results. I will select a few features based on my past experience and will re-rum the modeling processes.\n",
    ">\n",
    "> Once I complete the new models, I will evaluate their performance against my initial models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Prior to modeling, I will divide my data by a 75/25 split to have post-training validation data as my test set.\n",
    ">\n",
    "> I will use most of the default settings, with the addition of \"stratify = y,\" which will help preserve the class imbalance of the overall dataset in each of the splits.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:13.091114Z",
     "start_time": "2021-09-22T22:56:12.807084Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting data into features and target variables.\n",
    "target= 'is_canceled'\n",
    "\n",
    "X = data.drop(columns = [target]).copy()\n",
    "y = data[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:13.437087Z",
     "start_time": "2021-09-22T22:56:13.094084Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking for missing values\n",
    "print(f'Missing values for X:\\n {X.isna().sum()}\\n')\n",
    "print(f'Missing values for y: {y.isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:13.816083Z",
     "start_time": "2021-09-22T22:56:13.441086Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting - stratify to maintain class balance b/t X_train/_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:14.054084Z",
     "start_time": "2021-09-22T22:56:13.819095Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving memory by deleting unused X, y\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepping the Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    \n",
    "After splitting my data, I will create a pipeline to process my datasets via one-hot encoding and scaling to prepare for modeling.\n",
    "\n",
    "I will use a column transformer to process my categorical features separately from my continuous features. \n",
    "* I will use a OneHotEncoder on the categorical features to convert them into binary features for each category\n",
    "* I will use the StandardScaler transformer to scale my continuous features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:14.322697Z",
     "start_time": "2021-09-22T22:56:14.056084Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "num_cols = X_train.select_dtypes('number').columns.to_list()\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:14.589703Z",
     "start_time": "2021-09-22T22:56:14.325704Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "cat_cols = X_train.select_dtypes(include='object').columns.to_list()\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:14.870704Z",
     "start_time": "2021-09-22T22:56:14.591703Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating ColumnTransformer and sub-transformers for imputation and encoding\n",
    "\n",
    "### --- Creating column pipelines --- ###\n",
    "\n",
    "cat_pipe = Pipeline(steps=[('ohe', OneHotEncoder(handle_unknown='ignore',\n",
    "                                                 sparse=False))])\n",
    "\n",
    "num_pipe = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "### --- Instantiating the ColumnTransformer --- ###\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipe, num_cols),\n",
    "                  ('cat', cat_pipe, cat_cols)])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:16.891188Z",
     "start_time": "2021-09-22T22:56:14.873706Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Fitting feature preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Getting feature names from OHE\n",
    "ohe_cat_names = preprocessor.named_transformers_['cat'].named_steps['ohe'].get_feature_names(cat_cols)\n",
    "\n",
    "## Generating list for column index\n",
    "final_cols = [*num_cols, *ohe_cat_names]\n",
    "\n",
    "# final_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T22:56:17.651221Z",
     "start_time": "2021-09-22T22:56:16.893189Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Transform via the ColumnTransformer preprocessor and create new dataframe\n",
    "\n",
    "X_train_df = pd.DataFrame(preprocessor.transform(X_train),\n",
    "                             columns=final_cols, index=X_train.index)\n",
    "\n",
    "X_test_tf_df = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                            columns=final_cols, index=X_test.index)\n",
    "\n",
    "display(X_train_df.head(5),X_test_tf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Resampling via SMOTE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In my EDA notebook, I noted that my target feature has an imbalance of about 2:1 in favor of the negative class. I will use the SMOTE transformer on my training to oversample my positive class, helping to increase my models' performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:00:52.167386Z",
     "start_time": "2021-09-22T22:56:17.653192Z"
    }
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state = 42, n_jobs=-1)\n",
    "\n",
    "X_train_tf_df, y_train = smote.fit_sample(X_train_df,y_train)\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Baseline Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced accuracy score: 0.5\n",
    "> \n",
    "> Testing balanced accuracy score: 0.5\n",
    "> \n",
    "> * *The scores are the same size.*\n",
    ">\n",
    "> Training data log loss: 17.26\n",
    ">\n",
    "> Testing data log loss: 17.40\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> I use the `DummyClassifier` model as my baseline model. This model simply predicts the average value for my target, resulting in a score that is equivalent to random chance.\n",
    ">\n",
    "> One of my goals with my next models will be to exceed this performance, aiming to maximize the average precision of my models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:00:53.289475Z",
     "start_time": "2021-09-22T23:00:52.170300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating baseline classifier model\n",
    "\n",
    "base = DummyClassifier(random_state = 42)\n",
    "base.fit(X_train_tf_df, y_train)\n",
    "\n",
    "clf.evaluate_classification(base,X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Target Metric: Average Precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**My main metric for modeling will be the average precision of the model.** \n",
    "\n",
    "I specifically chose to focus on this metric as I want to minimize the number of false positives (classifying a reservation would cancel when it would actually arrive). \n",
    "\n",
    "False positives could result in higher costs: if a guest arrives at the hotel and you are over-sold, the hotel (usually) covers the cost of relocating the guest to a different hotel. This incurs the cost of relocation as well as the probable loss of business from that guest as they may feel that the hotel is unreliable.\n",
    "\n",
    "Additionally, I use the *average* metric as I know my test set's target values are still imbalanced. This is a better evaluation of the model's performance as it takes the imbalance into consideration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:02.056619Z",
     "start_time": "2021-09-22T23:00:53.291477Z"
    }
   },
   "outputs": [],
   "source": [
    "## LogReg Model\n",
    "logreg = LogisticRegression(max_iter = 500, random_state = 42, n_jobs=-1)\n",
    "\n",
    "logreg.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:03.183468Z",
     "start_time": "2021-09-22T23:01:02.058616Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(logreg, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'average precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:03.844323Z",
     "start_time": "2021-09-22T23:01:03.186325Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.plot_log_odds(logreg, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced accuracy score: 0.82\n",
    "> \n",
    "> Testing balanced accuracy score: 0.82\n",
    "> \n",
    "> * *The scores are the same size.*\n",
    ">\n",
    "> Training data log loss: 0.37\n",
    ">\n",
    "> Testing data log loss: 0.37\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> My standard logistic regression performs pretty well with an average precision of 80%. This performance is decent, but I will develop and train additional models to see if I can improve that score.\n",
    ">\n",
    "> The top and bottom three features for this model are mostly composed of months of the year - December/November for the highest log-odds of cancellations, respectively, and January/February for the second- and third-lowest log odds, respectively.\n",
    ">\n",
    ">\n",
    "> **These results would make sense when considering business travelers vs. leisure travelers:**\n",
    ">\n",
    ">\n",
    ">   * Leisure travelers may be less experienced travelers and their plans may change more easily, resulting in more cancellations.\n",
    ">       * The winter holidays would increase leisure travel and decrease business travel as people take time off of work. \n",
    ">\n",
    ">\n",
    ">   * Business travelers tend to be more experienced and their plans are more established\n",
    ">       * When the new year begins, people return to work, increasing business travel and decreasing leisure travel.\n",
    ">\n",
    "> **The remaining top/bottom features both require some caution to their interpretations:**\n",
    "* Reservations for room type \"P:\" due to the anonymous nature of the source data, it is hard to gain much, if any, significant insight from this feature.\n",
    "    * Room types may vary depending on size, amenities, etc.; without more information about the type of room, the feature has limited value.\n",
    "* Reservations requiring more parking: during my review of this feature in my EDA notebook, I noted my split feelings about including this feature: it is not guaranteed to be known in advance, and there are significantly more reservations without parking than reservations with parking.\n",
    "    * This feature is heavily dependent on whether a hotel reliably knows that a guest will need parking or not.\n",
    ">\n",
    "> I will continue with additional models to determine whether these results are the best for my recommendations and future modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:15.011235Z",
     "start_time": "2021-09-22T23:01:03.846324Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42,n_jobs=-1)\n",
    "\n",
    "rfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:18.721111Z",
     "start_time": "2021-09-22T23:01:15.013120Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(rfc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'average precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:19.337103Z",
     "start_time": "2021-09-22T23:01:18.726103Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.plot_importances(rfc, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training balanced recall score: 0.96\n",
    "> \n",
    "> Testing balanced recall score: 0.88\n",
    ">\n",
    "> * *The training score is larger by 0.08 points.*\n",
    ">\n",
    "> Training data log loss: 0.56\n",
    ">\n",
    "> Testing data log loss: 0.53\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> The Random Forest Classifier with the default settings shows a larger average precision than the logistic regression: 88% vs. 80%. I am more trusting of this model versus the logistic regression due to this increase in performance.\n",
    ">\n",
    "> This model uses feature importances to rank its results. Feature importances are limited in that they do not indicate whether a feature increases or decreases the likelihood of a reservation canceling, however we can use this information for further testing on new results.\n",
    ">\n",
    "> The feature importances rank reservations from Portugal and reservation lead times as the top most important features, with the number of special requests and deposit requirements filling out the remaining spots of the top five values.\n",
    ">\n",
    "> Interestingly, I see parking spaces was also important for this model, matching the logistic regression results. I am still uncertain about those results, however, and would continue to take them with a grain of salt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ExtraTreesClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:35.716119Z",
     "start_time": "2021-09-22T23:01:19.340121Z"
    }
   },
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(random_state=42,n_jobs = -1)\n",
    "\n",
    "etc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:40.054237Z",
     "start_time": "2021-09-22T23:01:35.718120Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(etc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'average precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:40.640294Z",
     "start_time": "2021-09-22T23:01:40.056250Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.plot_importances(etc, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training average precision score: 1.0\n",
    "> \n",
    "> Testing average precision score: 0.87\n",
    "> \n",
    "> * \n",
    ">\n",
    "> Training data log loss: 0.01\n",
    ">\n",
    "> Testing data log loss: 0.33\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> The Extra Tress Classifier showed similar results as the Random Forest Classifier in terms of scoring and feature importances, but at the expense of increased model complexity and slower run time.\n",
    ">\n",
    "> I will continue to consider my Random Forest Classifier as my best model considering its simpler nature and faster runtime - aspects that would have a greater impact with larger datasets.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:01:59.350314Z",
     "start_time": "2021-09-22T23:01:40.642294Z"
    }
   },
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:02:02.941467Z",
     "start_time": "2021-09-22T23:01:59.353315Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(xgbc, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'balanced recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:02:03.617320Z",
     "start_time": "2021-09-22T23:02:02.943332Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.plot_importances(xgbc, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Results:**\n",
    "\n",
    "> Training average precision score: 1.0\n",
    "> \n",
    "> Testing average precision score: 0.87\n",
    "> \n",
    "> * \n",
    ">\n",
    "> Training data log loss: 0.01\n",
    ">\n",
    "> Testing data log loss: 0.33\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> While the XGBoostClassifier model performed about the same as the Random Forest and Extra Trees Classifiers, I noticed that the feature importances are significantly different: **the XGBoost classifier ranks the non-refundable deposits above any other feature, almost to the point of being equivalent to the feature importances of all the other features combined!**\n",
    ">\n",
    "> The XGBoostClassifier still ranks Portugal, parking, and \"no deposit\" features highly, which supports the results of the other tree-based models. While the non-refundable feature importance is a surprising result, I find it somewhat strange to see it ranked so highly above the rest. Since the rest of the results match the results of the other tree-based models,  I will focus on those results.\n",
    ">\n",
    "> **Considering the similar results and slightly-worse performance of this model vs. the others, I will continue to stick with the RandomForest model as my best model.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T13:53:05.110549Z",
     "start_time": "2021-09-10T13:53:04.871349Z"
    }
   },
   "source": [
    "# **GridSearches and Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Hyperparameter Tuning**\n",
    "\n",
    "> After evaluating several model types, I decided that my RandomForestClassifier was my preferred model. **I will now use GridSearchCV to identify the best parameters for this model to maximize its average precision.**\n",
    ">\n",
    "> However, the feature importances of a tree-based model limit the interpretability, so I will also tune the logistic regression to see if I can improve its performance and re-evaluate its value in comparison to the RandomForestClassfier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:02:03.837362Z",
     "start_time": "2021-09-22T23:02:03.620323Z"
    }
   },
   "outputs": [],
   "source": [
    "lg_params = {\n",
    "    'max_iter': [500, 600, 700],\n",
    "    'C': [1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:05:57.622550Z",
     "start_time": "2021-09-22T23:02:03.839320Z"
    }
   },
   "outputs": [],
   "source": [
    "## LogReg Model\n",
    "lrgs = GridSearchCV(LogisticRegression(random_state = 42),lg_params,\n",
    "                    scoring = 'average_precision', verbose = 3)\n",
    "\n",
    "lrgs.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:05:58.698562Z",
     "start_time": "2021-09-22T23:05:57.624552Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(lrgs, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:05:58.934552Z",
     "start_time": "2021-09-22T23:05:58.701569Z"
    }
   },
   "outputs": [],
   "source": [
    "lrgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:05:59.169561Z",
     "start_time": "2021-09-22T23:05:58.936552Z"
    }
   },
   "outputs": [],
   "source": [
    "best_logreg = lrgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:05:59.786553Z",
     "start_time": "2021-09-22T23:05:59.171552Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.plot_log_odds(best_logreg, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Scores**\n",
    "\n",
    "> Training balanced accuracy score: .82\n",
    "> \n",
    "> Testing balanced accuracy score: .81\n",
    "> \n",
    "> * \n",
    ">\n",
    "> Training data log loss: .38\n",
    ">\n",
    "> Testing data log loss: .40\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**\n",
    "\n",
    "> {'C': 10, 'max_iter': 500}\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> It seems that my initial setting of 500 iterations was the best choice, and a weaker regularization strength of 10 performed better than the default of 1. However, neither the scores or coefficients showed any significant change.\n",
    ">\n",
    "> **I will keep these results in mind, but I still believe the RandomForestClassifer is the best model.**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:26.977039Z",
     "start_time": "2021-09-22T23:05:59.789556Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc_params = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [40,50, 60],\n",
    "              'min_samples_split': [2,3]\n",
    "}\n",
    "\n",
    "rfgs = GridSearchCV(RandomForestClassifier(random_state = 42, n_jobs=-1),\n",
    "                    rfc_params,scoring = 'average_precision',verbose = 3,\n",
    "                   cv = 3)\n",
    "\n",
    "rfgs.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:27.229046Z",
     "start_time": "2021-09-22T23:10:26.979030Z"
    }
   },
   "outputs": [],
   "source": [
    "rfgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:27.475078Z",
     "start_time": "2021-09-22T23:10:27.232032Z"
    }
   },
   "outputs": [],
   "source": [
    "best_rfc = rfgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:31.115035Z",
     "start_time": "2021-09-22T23:10:27.477033Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(best_rfc, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'average precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:31.791069Z",
     "start_time": "2021-09-22T23:10:31.117035Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## New Importances\n",
    "clf.plot_importances(best_rfc, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:32.427038Z",
     "start_time": "2021-09-22T23:10:31.793035Z"
    }
   },
   "outputs": [],
   "source": [
    "## Old Importances\n",
    "clf.plot_importances(rfc, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Scores**\n",
    "\n",
    "> Training balanced accuracy score: 1.0\n",
    "> \n",
    "> **Testing balanced accuracy score: .87**\n",
    "> \n",
    "> * *The training score is larger by 0.13 points.*\n",
    ">\n",
    "> Training data log loss: .05\n",
    ">\n",
    "> Testing data log loss: .30\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**\n",
    "\n",
    "> * criterion : entropy\n",
    "* max_depth : 60\n",
    "* min_samples_split : 2\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> The RandomForestClassfier best parameters changed slightly, with a few changes in the feature importances order, but the average precision stayed the same.\n",
    ">\n",
    "> The new parameters increased the importance of ADR by a few places, but the other top features remained relatively the same.\n",
    ">\n",
    "> Interestingly, despite the average precision not changing, I did notice two secondary metrics improved:\n",
    "* The log-loss metric decreased (.53 vs .28)\n",
    "* The AUC score increased (.79 vs. 95)\n",
    ">\n",
    "> While I am not focusing on those metrics, they do support the chosen hyperparameters and support my choice to use the RandomForestClassifier as my best model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS: Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To confirm my RandomForestClassifier results, I will compare it against a hyper-parameter-tuned ExtraTreesClassifier to ensure my results are in-line with the original results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:32.651034Z",
     "start_time": "2021-09-22T23:10:32.429037Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# etc_params = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [50, 60],\n",
    "#     'min_samples_leaf': [1,2,3]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:32.888484Z",
     "start_time": "2021-09-22T23:10:32.653038Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# etgs = GridSearchCV(ExtraTreesClassifier(random_state = 42, n_jobs=-1),etc_params,\n",
    "#                     scoring = 'average_precision', verbose = 2)\n",
    "\n",
    "# etgs.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:33.128330Z",
     "start_time": "2021-09-22T23:10:32.891303Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# etgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:33.366270Z",
     "start_time": "2021-09-22T23:10:33.130302Z"
    }
   },
   "outputs": [],
   "source": [
    "# best_etc = etgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:33.589434Z",
     "start_time": "2021-09-22T23:10:33.368273Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf.evaluate_classification(best_etc, X_train = X_train_tf_df,y_train = y_train,\n",
    "#                            X_test = X_test_tf_df, y_test = y_test,\n",
    "#                           metric = 'average precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:33.826289Z",
     "start_time": "2021-09-22T23:10:33.592291Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf.plot_importances(best_etc, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Scores**\n",
    "\n",
    "> Training balanced accuracy score: 1.0\n",
    "> \n",
    "> **Testing balanced accuracy score: .87**\n",
    "> \n",
    "> * *The training score is larger by 0.13 points.*\n",
    ">\n",
    "> Training data log loss: .05\n",
    ">\n",
    "> Testing data log loss: .30\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**\n",
    "\n",
    "> {'criterion': 'entropy', 'max_depth': 50, 'min_samples_leaf': 1}\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> Even with hyperparameter tuning, the ExtraTreesClassifier did not have better metrics than the RandomForestClassifier. Furthermore, it took *3x* longer to run - nearly 15 minutes in my case.\n",
    ">\n",
    "> Due to the runtime increase and lack of improvement in metrics, I will stick with the RandomForestClassifier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Collecting Coeffcients, Inspecting Importances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T14:26:58.848264Z",
     "start_time": "2021-09-08T14:26:58.725265Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "> After performing hyperparameter tuning, I determined my best model is the RandomForestClassfier. As a comparison, I will also review the final coefficients of my logistic regression model to see how the features match up.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:34.463980Z",
     "start_time": "2021-09-22T23:10:33.828288Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.plot_importances(best_rfc, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:35.087134Z",
     "start_time": "2021-09-22T23:10:34.465979Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.plot_log_odds(best_logreg, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The respective results for each model are very different, with only the `required_car_parking_spaces` overlapping between the two. **Due to the better scores and low runtime, I consider my RandomForestClassifier to be the best model.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interpreting Results with SHAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **One of the downsides of tree-based models is the difficulty when interpreting the impact of a specific feature.** Feature importances from tree-based models indicate how often a feature was used to make a decision, but they do not indicate if that feature was more or less likely to predict the target feature.\n",
    ">\n",
    "> To interpret these results, I will utilize a visualization package called **SHAP** to produce \"Shapely values\" for each feature. These values indicate each feature's marginal contribution to the model - answering the question, \"*How well does the model perform with this feature than without?*\"\n",
    ">\n",
    ">Using tools within the package, I will focus on the `summary_plot`, which visualizes each feature's Shapely value and the feature's  specific values from low-high (relative to each feature).\n",
    ">\n",
    "> More information about SHAP:\n",
    "* [SHAP Documentation](https://shap.readthedocs.io/en/latest/?badge=latest)\n",
    "* [SHAP Repository](https://github.com/slundberg/shap)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:35.399818Z",
     "start_time": "2021-09-22T23:10:35.089031Z"
    }
   },
   "outputs": [],
   "source": [
    " ## Initializing Javascript for SHAP models\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:35.648818Z",
     "start_time": "2021-09-22T23:10:35.401788Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generating a sample of the overall data for review:\n",
    "X_shap = shap.sample(X_test_tf_df, nsamples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TreeExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The SHAP package includes a few different \"Explainer\" objects to \"explain\" the results of different types of models. Since I used a RandomForestClassifier, I will use the \"TreeExplainer\" to calculate my SHAP values for plotting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:10:36.213287Z",
     "start_time": "2021-09-22T23:10:35.650788Z"
    }
   },
   "outputs": [],
   "source": [
    "## Initializing an explainer with the RandomForestClassifier model\n",
    "t_explainer = shap.TreeExplainer(rfgs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:16:06.748498Z",
     "start_time": "2021-09-22T23:10:36.215146Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculating SHAP values for sample test data\n",
    "shap_values = t_explainer.shap_values(X_shap)\n",
    "len(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Now that I have my SHAP values, I will use them to create a dot plot. This plot consists of three main elements. From left to right:\n",
    "* **Feature labels**: ranked from most to least impactful for classifying results\n",
    "* **Feature rows**: each row is a linear representation of the SHAP values for the sampled values for that feature\n",
    "\t* Feature scores may be positive/negative: positive values increase likelihood of a positive classification; negatives decrease the likelihood\n",
    "\t* Features close to zero have little impact on the model\n",
    "* **Dot colors**: showing magnitude of the values; low values are blue, red values are high\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:16:07.152475Z",
     "start_time": "2021-09-22T23:16:06.750484Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Preparing column names for visualization labels\n",
    "X_shap.rename(columns = lambda x: x.title().replace('_', ' '), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:16:07.927476Z",
     "start_time": "2021-09-22T23:16:07.154472Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Better plot\n",
    "shap.summary_plot(shap_values[1],X_shap,max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Based on the results of my RandomForestClassifier and visualization of my SHAP plot, I see that...\n",
    "* Reservations from Portugal are more likely to cancel vs. reservations from elsewhere\n",
    "* Reservations with no special requests are more likely to cancel - however any amount of requests may decrease likelihood of cancellation\n",
    "* Higher lead times show a slightly positive relationship with cancellations\n",
    "* Reservations listed not as \"non-refundable\" are less likely to cancel\n",
    "* Reservations listed as \"no deposit\" are less likely to cancel\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Based on these results, I recommend the following:**\n",
    "* Contact guests with long lead times to confirm bookings\n",
    "* Monitor bookings from Portugal versus other countries\n",
    "* Limit availability of non-refundable rates to prevent/limit risk of cancellations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the future, I would like to improve this project by:\n",
    "* Including whether a reservation is booked over a major holiday to see if they are more/less likely to cancel\n",
    "* Develop time series forecasts using the daily average cancellation probabilities\n",
    "* Expanding the forecasts with vector autoregressive forecasting techniques - using more than just probabilities as factors for forecasts\n",
    "* Determining the likelihood of cancellations within particular windows of time prior to arrival\n",
    "    * E.g. 0-3, 4-7, or 7-14 days\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addendum: Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Now that I built and trained my ideal model for predicting cancellations, I will use the probabilities for each observation for time series modeling and forecasting.\n",
    ">\n",
    "> In this section, I will simply generate the probabilities required for the time series modeling. Once generated, I will append them to the original dataframe and save it as a new pickled file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:30:12.953338Z",
     "start_time": "2021-09-22T23:30:12.508337Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Reloading the full dataset\n",
    "\n",
    "data = pd.read_pickle('./data/data_no_assigned.pickle',\n",
    "                           compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:30:41.589692Z",
     "start_time": "2021-09-22T23:30:41.333155Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting data into features and target variable\n",
    "target= 'is_canceled'\n",
    "\n",
    "X = data.drop(columns = [target]).copy()\n",
    "y = data[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting feature preprocessor\n",
    "preprocessor.fit(X)\n",
    "\n",
    "## Getting feature names from OHE\n",
    "ohe_cat_names = preprocessor.named_transformers_['cat'].named_steps['ohe']\\\n",
    "                                                  .get_feature_names(cat_cols)\n",
    "\n",
    "## Generating list for column index\n",
    "final_cols = [*num_cols, *ohe_cat_names]\n",
    "\n",
    "## Transform via the ColumnTransformer preprocessor and create new dataframe\n",
    "X_tf_df = pd.DataFrame(preprocessor.transform(X),\n",
    "                             columns=final_cols, index=X.index)\n",
    "\n",
    "X_tf_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:32:55.253888Z",
     "start_time": "2021-09-22T23:32:54.181921Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generating probabilities via best model\n",
    "\n",
    "y_preds = best_rfc.predict_proba(X_tf_df)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:41:03.088863Z",
     "start_time": "2021-09-22T23:41:02.245720Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating list of \"positive\" probabilites\n",
    "\n",
    "proba_cxl = []\n",
    "for i in range(len(y_preds)):\n",
    "    proba_cxl.append(round(y_preds[i][1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:44:58.710036Z",
     "start_time": "2021-09-22T23:44:58.454036Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting list to Series and setting Series name\n",
    "pos_cxl = pd.Series(proba_cxl)\n",
    "pos_cxl.rename('cxl_probability', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:47:41.340242Z",
     "start_time": "2021-09-22T23:47:41.059243Z"
    }
   },
   "outputs": [],
   "source": [
    "## Appending probabilities to original dataframe\n",
    "data_probs = pd.concat([data, pos_cxl], axis=1)\n",
    "data_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T23:47:46.998252Z",
     "start_time": "2021-09-22T23:47:41.942245Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving the updated dataframe as a new pickle file\n",
    "## Pickling with Pandas\n",
    "data_probs.to_pickle(path = './data/data_probs.pickle',\n",
    "            compression = 'gzip')\n",
    "print(f'Successfully pickled!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_intel_env]",
   "language": "python",
   "name": "conda-env-test_intel_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
